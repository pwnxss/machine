#Gradient Descent Optimization

import numpy as np

# Define the gradient function for f(x) = x^2
def gradient(x):
    return 2 * x

# Gradient descent optimization function
def gradient_descent(gradient, start, learn_rate, n_iter=50, tolerance=1e-06):
    vector = start
    for _ in range(n_iter):
        diff = -learn_rate * gradient(vector)
        if np.all(np.abs(diff) <= tolerance):
            break
        vector += diff
    return vector

# Initial point
start = 5.0
# Learning rate
learn_rate = 0.1
# Number of iterations
n_iter = 50
# Tolerance for convergence
tolerance = 1e-6

# Gradient descent optimization
result = gradient_descent(gradient, start, learn_rate, n_iter, tolerance)
print(result)


#Bayesian Optimization

# First, ensure you have the necessary library installed:
# pip install scikit-optimize

from skopt import gp_minimize
from skopt.space import Real

# Define the function to be minimized
def objective_function(x):
    return (x[0] - 2) * 2 + (x[1] - 3) * 2 + 1

# Define the dimensions (search space)
dimensions = [Real(-5.0, 5.0), Real(-5.0, 5.0)]

# Implement Bayesian Optimization
def bayesian_optimization(func, dimensions, n_calls=50):
    result = gp_minimize(func, dimensions, n_calls=n_calls)
    return result.x, result.fun

# Run Bayesian Optimization
best_params, best_score = bayesian_optimization(objective_function, dimensions)

# Output the best parameters and the corresponding function value
print("Best parameters:", best_params)
print("Best score:", best_score)
